"""
√ö≈ôedn√≠ deska Chatbot - Kompletn√≠ ≈ôe≈°en√≠ s AI
Sbƒõr dat z NKOD -> Zpracov√°n√≠ dokument≈Ø -> RAG + LLM -> Chatbot
"""

import os
import json
import time
import hashlib
import re
import unicodedata
import requests
from datetime import datetime
from typing import List, Dict, Optional

# Optional dependencies
try:
    import PyPDF2
    pdf_available = True
except ImportError:
    pdf_available = False
    print("‚ö†Ô∏è PyPDF2 nen√≠ nainstalov√°n - PDF dokumenty nebudou zpracov√°ny")

# ============================================================================
# 1. SBƒöR DAT Z NKOD
# ============================================================================

class NKODCollector:
    """
    Sbƒõraƒç metadata √∫≈ôedn√≠ch desek z N√°rodn√≠ho katalogu otev≈ôen√Ωch dat (NKOD).
    
    Attributes:
        cache_dir: Adres√°≈ô pro cache soubor≈Ø
        sparql_endpoint: URL SPARQL endpointu
    """
    
    def __init__(self, cache_dir: str = "./nkod_cache") -> None:
        self.cache_dir = cache_dir
        self.sparql_endpoint = "https://data.gov.cz/sparql"
        os.makedirs(cache_dir, exist_ok=True)
    
    def search_boards(self, municipality: str, limit: int = 50) -> List[Dict]:
        """Vyhled√° √∫≈ôedn√≠ desky pro danou obec v NKOD"""
        
        # SPARQL dotaz pro vyhled√°n√≠ √∫≈ôedn√≠ch desek
        sparql_query = f"""
        SELECT ?dataset ?title ?description ?distribution ?url
        WHERE {{
            ?dataset a dcat:Dataset ;
                dcterms:title ?title ;
                dcat:distribution ?dist .
            
            ?dataset dcterms:subject|dcat:keyword ?keyword .
            ?dist dcat:accessURL ?url .
            
            FILTER (
                CONTAINS(LCASE(?title), "√∫≈ôedn√≠ deska") ||
                CONTAINS(LCASE(?title), "deska") ||
                CONTAINS(LCASE(?description), "√∫≈ôedn√≠")
            )
            FILTER (
                CONTAINS(LCASE(?title), "{municipality.lower()}") ||
                CONTAINS(LCASE(str(?dataset)), "{municipality.lower()}")
            )
            
            OPTIONAL {{ ?dataset dcterms:description ?description }}
            OPTIONAL {{ ?dist dcterms:format ?format }}
        }}
        LIMIT {limit}
        """
        
        try:
            response = requests.get(
                self.sparql_endpoint,
                params={"query": sparql_query, "format": "json"},
                timeout=10
            )
            response.raise_for_status()
            results = response.json().get("results", {}).get("bindings", [])
            return self._parse_results(results, municipality)
        except Exception as e:
            print(f"Chyba p≈ôi dotazu NKOD: {e}")
            return self._fallback_search(municipality)
    
    def _parse_results(self, results: List[Dict], municipality: str) -> List[Dict]:
        """Parsuje v√Ωsledky SPARQL dotazu"""
        datasets = {}
        
        for result in results:
            dataset_id = result.get("dataset", {}).get("value", "")
            if dataset_id in datasets:
                continue
            
            datasets[dataset_id] = {
                "id": dataset_id,
                "title": result.get("title", {}).get("value", ""),
                "description": result.get("description", {}).get("value", ""),
                "url": result.get("url", {}).get("value", ""),
                "format": result.get("format", {}).get("value", ""),
                "municipality": municipality,
                "fetched_at": datetime.now().isoformat()
            }
        
        return list(datasets.values())
    
    def _fallback_search(self, municipality: str) -> List[Dict]:
        """Fallback: Vyhled√°n√≠ p≈ôes web NKOD"""
        print(f"Zkou≈°√≠m fallback vyhled√°v√°n√≠ pro {municipality}...")
        try:
            response = requests.get(
                "https://data.gov.cz/datov√©-sady",
                params={"q": f"{municipality} √∫≈ôedn√≠ deska"},
                timeout=10
            )
            return []
        except Exception as e:
            print(f"Fallback selhalo: {e}")
            return []
    
    def cache_results(self, municipality: str, results: List[Dict]):
        """Cachuje v√Ωsledky lok√°lnƒõ"""
        cache_file = os.path.join(
            self.cache_dir,
            f"{municipality.lower().replace(' ', '_')}_metadata.json"
        )
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"Metadata ulo≈æena: {cache_file}")
    
    def load_cached(self, municipality: str) -> Optional[List[Dict]]:
        """Naƒçte cachovan√© v√Ωsledky"""
        cache_file = os.path.join(
            self.cache_dir,
            f"{municipality.lower().replace(' ', '_')}_metadata.json"
        )
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None


# ============================================================================
# 2. STA≈ΩEN√ç A ZPRACOV√ÅN√ç DOKUMENT≈Æ
# ============================================================================

class DocumentProcessor:
    """
    Procesor pro stahov√°n√≠ a zpracov√°n√≠ dokument≈Ø z √∫≈ôedn√≠ch desek.
    
    Podporuje JSON (√∫≈ôedn√≠ desky) a PDF soubory s inteligentn√≠ detekc√≠ form√°tu.
    
    Attributes:
        cache_dir: Adres√°≈ô pro sta≈æen√© soubory
    """
    
    def __init__(self, cache_dir: str = "./documents") -> None:
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def download_document(self, url: str, doc_id: str) -> Optional[str]:
        """St√°hne dokument a vr√°t√≠ cestu k souboru"""
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', 'application/octet-stream')
            ext = self._get_extension(content_type, url)
            
            filepath = os.path.join(self.cache_dir, f"{doc_id}{ext}")
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"Sta≈æen: {filepath} ({len(response.content)} bytes)")
            return filepath
        except Exception as e:
            print(f"Chyba p≈ôi sta≈æen√≠ {url}: {e}")
            return None
    
    def _get_extension(self, content_type: str, url: str) -> str:
        """Urƒç√≠ p≈ô√≠ponu souboru"""
        ext_map = {
            'application/pdf': '.pdf',
            'text/plain': '.txt',
            'application/json': '.json',
            'text/xml': '.xml',
            'application/xml': '.xml',
        }
        
        for ct, ext in ext_map.items():
            if ct in content_type:
                return ext
        
        if '.pdf' in url:
            return '.pdf'
        return '.bin'
    
    def extract_text(self, filepath: str) -> str:
        """Extrahuje text z dokumentu"""
        if filepath.endswith('.pdf') and pdf_available:
            return self._extract_pdf_text(filepath)
        elif filepath.endswith('.txt'):
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        elif filepath.endswith('.json') or filepath.endswith('.bin'):
            return self._extract_official_board_content(filepath)
        else:
            return f"[Nepodporovan√Ω form√°t: {filepath}]"
    
    def _extract_pdf_text(self, filepath: str) -> str:
        """Extrahuje text z PDF"""
        if not pdf_available:
            return "[PyPDF2 nen√≠ nainstalov√°n - PDF nelze zpracovat]"
        
        try:
            import PyPDF2  # Import here to avoid issues if not available
            text = []
            with open(filepath, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text.append(page.extract_text())
            return '\n'.join(text)
        except Exception as e:
            print(f"Chyba p≈ôi extrakci PDF: {e}")
            return "[Chyba p≈ôi extrakci textu z PDF]"
    
    def _extract_official_board_content(self, filepath: str) -> str:
        """Extrahuje strukturovan√Ω obsah z JSON soubor≈Ø √∫≈ôedn√≠ch desek"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            extracted_parts = []
            
            # Informace o provozovateli
            if 'provozovatel' in data and isinstance(data['provozovatel'], dict):
                prov = data['provozovatel']
                if 'n√°zev' in prov:
                    nazev = prov['n√°zev']
                    if isinstance(nazev, dict) and 'cs' in nazev:
                        extracted_parts.append(f"√ö≈òAD: {nazev['cs']}")
                    elif isinstance(nazev, str):
                        extracted_parts.append(f"√ö≈òAD: {nazev}")
            
            # Extrakce v≈°ech ozn√°men√≠/dokument≈Ø
            if 'informace' in data and isinstance(data['informace'], list):
                extracted_parts.append(f"\nDOKUMENTY NA √ö≈òEDN√ç DESCE ({len(data['informace'])} polo≈æek):")
                extracted_parts.append("=" * 50)
                
                for i, info in enumerate(data['informace'], 1):
                    if not isinstance(info, dict):
                        continue
                    
                    doc_parts = [f"\n[DOKUMENT {i}]"]
                    
                    # N√°zev dokumentu/ozn√°men√≠
                    if 'n√°zev' in info:
                        nazev = info['n√°zev']
                        if isinstance(nazev, dict) and 'cs' in nazev:
                            doc_parts.append(f"N√ÅZEV: {nazev['cs']}")
                        elif isinstance(nazev, str):
                            doc_parts.append(f"N√ÅZEV: {nazev}")
                    
                    # Popis/anotace
                    if 'anotace' in info:
                        anotace = info['anotace']
                        if isinstance(anotace, dict) and 'cs' in anotace:
                            doc_parts.append(f"OBSAH: {anotace['cs']}")
                        elif isinstance(anotace, str):
                            doc_parts.append(f"OBSAH: {anotace}")
                    
                    # Typ dokumentu
                    if 'typ' in info:
                        if isinstance(info['typ'], list):
                            doc_parts.append(f"TYP: {', '.join(info['typ'])}")
                        elif isinstance(info['typ'], str):
                            doc_parts.append(f"TYP: {info['typ']}")
                    
                    # Datum schv√°len√≠
                    if 'schv√°leno' in info:
                        schvaleni = info['schv√°leno']
                        if isinstance(schvaleni, dict) and 'datum_a_ƒças' in schvaleni:
                            datum = schvaleni['datum_a_ƒças']
                            try:
                                # Form√°tov√°n√≠ data
                                dt = datetime.fromisoformat(datum.replace('Z', '+00:00'))
                                formatted_date = dt.strftime('%d.%m.%Y')
                                doc_parts.append(f"SCHV√ÅLENO: {formatted_date}")
                            except:
                                doc_parts.append(f"SCHV√ÅLENO: {datum}")
                    
                    # Platnost do
                    if 'relevantn√≠_do' in info:
                        rel_do = info['relevantn√≠_do']
                        if isinstance(rel_do, dict) and 'datum_a_ƒças' in rel_do:
                            datum = rel_do['datum_a_ƒças']
                            try:
                                dt = datetime.fromisoformat(datum.replace('Z', '+00:00'))
                                formatted_date = dt.strftime('%d.%m.%Y')
                                doc_parts.append(f"PLATN√â DO: {formatted_date}")
                            except:
                                doc_parts.append(f"PLATN√â DO: {datum}")
                    
                    # Agenda/kategorie
                    if 'agenda' in info and isinstance(info['agenda'], list):
                        agendy = []
                        for agenda in info['agenda']:
                            if isinstance(agenda, dict) and 'n√°zev' in agenda:
                                nazev_agendy = agenda['n√°zev']
                                if isinstance(nazev_agendy, dict) and 'cs' in nazev_agendy:
                                    agendy.append(nazev_agendy['cs'])
                                elif isinstance(nazev_agendy, str):
                                    agendy.append(nazev_agendy)
                        if agendy:
                            doc_parts.append(f"KATEGORIE: {', '.join(agendy)}")
                    
                    # URL na detail
                    if 'url' in info:
                        doc_parts.append(f"ODKAZ: {info['url']}")
                    
                    extracted_parts.append('\n'.join(doc_parts))
            
            result = '\n'.join(extracted_parts)
            return result if result.strip() else "[≈Ω√°dn√Ω obsah k extrakci]"
            
        except json.JSONDecodeError as e:
            print(f"JSON parse error v {filepath}: {e}")
            return "[Chyba: Neplatn√Ω JSON form√°t]"
        except Exception as e:
            print(f"Chyba p≈ôi extrakci obsahu z {filepath}: {e}")
            return f"[Chyba p≈ôi extrakci: {e}]"

    def process_documents(self, metadata_list: List[Dict]) -> List[Dict]:
        """Zpracuje v≈°echny dokumenty z metadat"""
        processed = []
        
        for meta in metadata_list:
            url = meta.get('url', '')
            if not url:
                continue
            
            doc_id = hashlib.md5(url.encode()).hexdigest()[:8]
            filepath = self.download_document(url, doc_id)
            
            if filepath:
                text = self.extract_text(filepath)
                processed.append({
                    "doc_id": doc_id,
                    "title": meta.get('title', ''),
                    "municipality": meta.get('municipality', ''),
                    "original_url": url,
                    "filepath": filepath,
                    "text": text,
                    "processed_at": datetime.now().isoformat()
                })
            
            time.sleep(0.5)
        
        return processed


# ============================================================================
# 3. VEKTOROV√â ULO≈ΩI≈†Tƒö A RAG
# ============================================================================

class SimpleVectorStore:
    """
    Vektorov√© √∫lo≈æi≈°tƒõ pro dokumenty s pokroƒçil√Ωm ƒçesk√Ωm vyhled√°v√°n√≠m.
    
    Implementuje RAG (Retrieval-Augmented Generation) s podporou ƒçesk√© morfologie
    a inteligentn√≠m sk√≥rov√°n√≠m relevance.
    
    Attributes:
        store_file: Soubor pro perzistenci dat
        documents: Seznam v≈°ech dokument≈Ø
    """
    
    def __init__(self, store_file: str = "vector_store.json") -> None:
        self.store_file = store_file
        self.documents: List[Dict] = []
    
    def add_documents(self, documents: List[Dict]):
        """P≈ôid√° dokumenty do √∫lo≈æi≈°tƒõ"""
        self.documents.extend(documents)
        self.save()
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """Pokroƒçil√© vyhled√°v√°n√≠ s podporou ƒçe≈°tiny a AI optimalizacemi"""
        
        def normalize_czech_text(text):
            """Pokroƒçil√° normalizace ƒçesk√©ho textu"""
            if not text:
                return ""
            text = text.lower().strip()
            
            # Mapa ƒçesk√Ωch morfologick√Ωch variant
            czech_variants = {
                'dƒõƒç√≠n': ['decin', 'decine', 'decina', 'decinem', 'decinu'],
                'ostrava': ['ostrave', 'ostravu', 'ostravou', 'ostravy'],
                'praha': ['praze', 'prahu', 'prahou', 'prahy'],
                'brno': ['brne', 'brnu', 'brnem', 'brna'],
                'teplice': ['teplicich', 'teplici', 'teplicemi'],
                '√∫st√≠': ['usti', 'ustim', 'ustich']
            }
            
            # Odstranƒõn√≠ diakritiky
            text = unicodedata.normalize('NFD', text)
            text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
            
            # Roz≈°√≠≈ôen√≠ o morfologick√© varianty
            expanded_text = text
            for base_form, variants in czech_variants.items():
                for variant in variants:
                    if variant in text:
                        expanded_text += ' ' + base_form
            
            return expanded_text
        
        def calculate_relevance_score(doc, query_terms, query_normalized):
            """Pokroƒçil√© bodov√°n√≠ relevance"""
            score = 0.0
            
            # Z√≠skej a normalizuj text dokumentu
            doc_texts = {
                'title': normalize_czech_text(doc.get('title', '')),
                'municipality': normalize_czech_text(doc.get('municipality', '')),
                'text': normalize_czech_text(doc.get('text', '')),
                'url': normalize_czech_text(doc.get('original_url', ''))
            }
            
            # V√°hy pro r≈Øzn√© pole
            field_weights = {
                'municipality': 10.0,  # Nejvy≈°≈°√≠ v√°ha pro obec
                'title': 5.0,          # Vysok√° v√°ha pro n√°zev
                'text': 1.0,           # Standardn√≠ v√°ha pro obsah
                'url': 0.5             # N√≠zk√° v√°ha pro URL
            }
            
            # Bodov√°n√≠ podle jednotliv√Ωch slov
            for term in query_terms:
                if len(term) < 2:
                    continue
                
                for field, field_text in doc_texts.items():
                    if not field_text:
                        continue
                    
                    weight = field_weights.get(field, 1.0)
                    
                    # P≈ôesn√° shoda (nejvy≈°≈°√≠ sk√≥re)
                    exact_matches = field_text.count(term)
                    score += exact_matches * weight * 3
                    
                    # ƒå√°steƒçn√° shoda (fuzzy matching)
                    if term in field_text:
                        score += weight * 1.5
                    
                    # Podobnost slov (Levenshtein-like)
                    for word in field_text.split():
                        if len(word) > 2 and term in word:
                            similarity = len(term) / len(word)
                            score += weight * similarity
            
            # Bonus pro ƒçerstv√© dokumenty
            try:
                processed_date = doc.get('processed_at', '')
                if processed_date:
                    doc_date = datetime.fromisoformat(processed_date.replace('Z', '+00:00'))
                    days_old = (datetime.now(doc_date.tzinfo) - doc_date).days
                    freshness_bonus = max(0, 1 - (days_old / 365))  # Bonus kles√° s vƒõkem
                    score += freshness_bonus * 0.5
            except:
                pass
            
            # Bonus pro dokumenty s obsahem (ne jen "[Nepodporovan√Ω form√°t]")
            if doc.get('text', '') and not doc.get('text', '').startswith('[Nepodporovan√Ω'):
                score += 1.0
            
            return max(score, 0.1) if score > 0 else 0
        
        # P≈ô√≠prava vyhled√°v√°n√≠
        query_normalized = normalize_czech_text(query)
        query_terms = [term for term in query_normalized.split() if len(term) > 1]
        
        if not query_terms:
            return []
        
        # V√Ωpoƒçet sk√≥re pro v≈°echny dokumenty
        scored_docs = []
        for doc in self.documents:
            score = calculate_relevance_score(doc, query_terms, query_normalized)
            if score > 0:
                scored_docs.append((score, doc))
        
        # Se≈ôazen√≠ podle relevance
        scored_docs.sort(reverse=True, key=lambda x: x[0])
        
        # Inteligentn√≠ v√Ωbƒõr dokument≈Ø
        if any(word in query.lower() for word in ['obce', 'mƒõsta', 'pokr√Ωv√°te', 'dostupn√©', 'jak√©']):
            # Pro obecn√© dotazy - diverzita obc√≠
            selected_docs = []
            used_municipalities = set()
            
            for score, doc in scored_docs:
                municipality = doc.get('municipality', 'Unknown')
                if municipality not in used_municipalities or len(selected_docs) < 3:
                    selected_docs.append(doc)
                    used_municipalities.add(municipality)
                    if len(selected_docs) >= top_k:
                        break
            
            return selected_docs[:top_k]
        else:
            # Pro specifick√© dotazy - nejrelevantnƒõj≈°√≠ dokumenty
            return [doc for _, doc in scored_docs[:top_k]]
    
    def save(self):
        """Ulo≈æ√≠ √∫lo≈æi≈°tƒõ"""
        with open(self.store_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def load(self):
        """Naƒçte √∫lo≈æi≈°tƒõ"""
        if os.path.exists(self.store_file):
            with open(self.store_file, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)


class OllamaClient:
    """
    Klient pro komunikaci s Ollama API.
    
    Poskytuje rozhran√≠ pro generov√°n√≠ odpovƒõd√≠ pomoc√≠ lok√°ln√≠ch LLM model≈Ø
    bƒõ≈æ√≠c√≠ch v Ollama serveru.
    
    Attributes:
        model: N√°zev modelu (nap≈ô. "gemma3:4b")
        base_url: URL Ollama serveru
        chat_url: Endpoint pro chat API
    """
    
    def __init__(self, model: str = "gemma3:4b", base_url: str = "http://localhost:11434") -> None:
        self.model = model
        self.base_url = base_url
        self.chat_url = f"{base_url}/api/chat"
        self._check_connection()
    
    def _check_connection(self):
        """Zkontroluje, zda je Ollama dostupn√°"""
        print(f"üîç Testuji p≈ôipojen√≠ k Ollama na {self.base_url}...")
        
        # Test pouze kl√≠ƒçov√© API
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                print(f"  ‚úì P≈ôipojeno! Dostupn√© modely: {', '.join(model_names)}")
                
                if not any(self.model in name for name in model_names):
                    print(f"  ‚ö† Model {self.model} nen√≠ sta≈æen√Ω!")
                    print(f"    St√°hnƒõte: ollama pull {self.model}")
                else:
                    print(f"  ‚úì Model {self.model} je p≈ôipraven")
            else:
                print(f"  ‚ö† Neoƒçek√°van√° odpovƒõƒè: {response.status_code}")
        except Exception as e:
            print(f"  ‚ö† P≈ôipojen√≠ se nezda≈ôilo: {e}")
            print(f"  Ujistƒõte se, ≈æe Ollama bƒõ≈æ√≠: ollama serve")
    
    def generate(self, prompt: str, system: Optional[str] = None, stream: bool = False) -> str:
        """Generuje odpovƒõƒè pomoc√≠ Ollama"""
        messages = []
        
        if system:
            messages.append({"role": "system", "content": system})
        
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": 0.1,
                "num_predict": 512
            }
        }
        
        try:
            response = requests.post(
                self.chat_url,
                json=payload,
                timeout=60,
                stream=stream
            )
            response.raise_for_status()
            
            if stream:
                return self._handle_stream(response)
            else:
                # Handle non-streaming response - Ollama returns multiple JSON objects
                content_parts = []
                response_text = response.text.strip()
                
                # Split by lines and parse each JSON object
                for line in response_text.split('\n'):
                    if line.strip():
                        try:
                            data = json.loads(line)
                            if 'message' in data and 'content' in data['message']:
                                content_parts.append(data['message']['content'])
                        except json.JSONDecodeError:
                            continue
                
                return ''.join(content_parts)
        
        except requests.exceptions.Timeout:
            return "Chyba: ƒåasov√Ω limit vypr≈°el. Zkuste krat≈°√≠ dotaz."
        except Exception as e:
            return f"Chyba p≈ôi komunikaci s Ollama: {e}"
    
    def _handle_stream(self, response):
        """Zpracuje streamovanou odpovƒõƒè"""
        full_response = []
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line)
                    if 'message' in data and 'content' in data['message']:
                        full_response.append(data['message']['content'])
                except json.JSONDecodeError:
                    continue
        return ''.join(full_response)


class ChatbotRAG:
    """
    Pokroƒçil√Ω RAG chatbot pro ƒçeskou ve≈ôejnou spr√°vu.
    
    Vyu≈æ√≠v√° Ollama s modelem Gemma 3:4b pro generov√°n√≠ odpovƒõd√≠
    na z√°kladƒõ dokument≈Ø z √∫≈ôedn√≠ch desek.
    
    Attributes:
        vector_store: √ölo≈æi≈°tƒõ dokument≈Ø
        ollama: Klient pro komunikaci s Ollama
        conversation_history: Historie konverzace
        system_prompt: Syst√©mov√Ω prompt pro AI
    """
    
    def __init__(self, vector_store: SimpleVectorStore, model: str = "gemma3:4b") -> None:
        self.vector_store = vector_store
        self.ollama = OllamaClient(model=model)
        self.conversation_history = []
        
        self.system_prompt = """Jsi vysoce pokroƒçil√Ω AI asistent specializovan√Ω na ƒçeskou ve≈ôejnou spr√°vu a √∫≈ôedn√≠ desky.

TVOJE ROLE:
- Expert na ƒçesk√© pr√°vo a ve≈ôejnou spr√°vu
- Odborn√≠k na √∫≈ôedn√≠ dokumenty a jejich interpretaci
- Pomocn√≠k obƒçan≈Ø p≈ôi orientaci v √∫≈ôedn√≠ch z√°le≈æitostech

ZP≈ÆSOB ODPOV√çD√ÅN√ç:
- Pou≈æ√≠vej POUZE informace z poskytnut√Ωch dokument≈Ø
- Odpov√≠dej v ƒçe≈°tinƒõ s korektn√≠ gramatikou
- Buƒè p≈ôesn√Ω, konkr√©tn√≠ a u≈æiteƒçn√Ω
- Strukturuj odpovƒõdi p≈ôehlednƒõ (odr√°≈æky, ƒç√≠slov√°n√≠)
- Vysvƒõtli kontext a d≈Øsledky pro obƒçany

SPECI√ÅLN√ç INSTRUKCE:
- Pro dotazy na obce/mƒõsta: vypi≈° V≈†ECHNY obce z dokument≈Ø
- Pro konkr√©tn√≠ dotazy: poskytni detailn√≠ informace s odkazy na dokumenty
- Pokud nejsou informace dostupn√©, navrhni alternativn√≠ zdroje
- Identifikuj typ dokumentu (vyhl√°≈°ka, ozn√°men√≠, rozpoƒçet, atd.)
- Upozorni na d≈Øle≈æit√© term√≠ny a lh≈Øty"""
    
    def generate_response(self, user_query: str) -> str:
        """Pokroƒçil√° generace odpovƒõdi s AI optimalizacemi"""
        
        # 1. Anal√Ωza typu dotazu
        query_type = self._analyze_query_type(user_query)
        
        # 2. Adaptivn√≠ vyhled√°v√°n√≠ podle typu dotazu
        top_k = self._determine_search_depth(query_type, user_query)
        relevant_docs = self.vector_store.search(user_query, top_k=top_k)
        
        if not relevant_docs:
            return self._generate_no_results_response(user_query)
        
        # 3. Inteligentn√≠ sestaven√≠ kontextu
        context = self._build_enhanced_context(relevant_docs, query_type)
        
        # 4. Dynamick√© vytvo≈ôen√≠ promtu podle typu dotazu
        prompt = self._build_adaptive_prompt(user_query, context, query_type)
        
        # 5. Optimalizovan√© vol√°n√≠ AI modelu
        print(f"\nü§ñ Vol√°m Gemma 3:4b (dotaz typu: {query_type})...")
        response = self.ollama.generate(prompt, system=self.system_prompt)
        
        # 6. Post-processing odpovƒõdi
        enhanced_response = self._enhance_response(response, relevant_docs, query_type)
        
        # 7. Ulo≈æen√≠ do historie s metadaty
        self.conversation_history.append({
            "user": user_query,
            "assistant": enhanced_response,
            "sources": [doc.get('title', '') for doc in relevant_docs],
            "query_type": query_type,
            "doc_count": len(relevant_docs),
            "timestamp": datetime.now().isoformat()
        })
        
        return enhanced_response
    
    def _build_context(self, documents: List[Dict]) -> str:
        """Sestroj√≠ kontext z dokument≈Ø"""
        context_parts = []
        for i, doc in enumerate(documents, 1):
            snippet = doc.get('text', '')[:800]  # Prvn√≠ 800 znak≈Ø
            title = doc.get('title', 'Dokument bez n√°zvu')
            municipality = doc.get('municipality', '')
            
            context_parts.append(
                f"[Dokument {i}]\n"
                f"N√°zev: {title}\n"
                f"Obec: {municipality}\n"
                f"Obsah: {snippet}..."
            )
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str) -> str:
        """Sestav√≠ prompt pro Gemma"""
        # Speci√°ln√≠ instrukce pro dotazy o obc√≠ch/mƒõstech
        if any(word in query.lower() for word in ['obce', 'mƒõsta', 'pokr√Ωv√°te', 'dostupn√©', 'jak√©']):
            instruction = "D≈ÆLE≈ΩIT√â: Pokud se ptaj√≠ na dostupn√© obce/mƒõsta, vypi≈° V≈†ECHNY obce uveden√© v dokumentech n√≠≈æe:"
        else:
            instruction = "Na z√°kladƒõ n√°sleduj√≠c√≠ch dokument≈Ø z √∫≈ôedn√≠ch desek odpovƒõz na ot√°zku u≈æivatele."
        
        return f"""{instruction}

DOKUMENTY:
{context}

OT√ÅZKA U≈ΩIVATELE:
{query}

ODPOVƒöƒé (struƒçnƒõ a na z√°kladƒõ dokument≈Ø):"""
    
    def _analyze_query_type(self, query: str) -> str:
        """Analyzuje typ dotazu pro optimalizaci odpovƒõdi"""
        query_lower = query.lower()
        
        # Obecn√© dotazy o pokryt√≠
        if any(word in query_lower for word in ['obce', 'mƒõsta', 'pokr√Ωv√°te', 'dostupn√©', 'jak√©']):
            return 'coverage'
        
        # Dotazy o konkr√©tn√≠ obci
        if any(city in query_lower for city in ['dƒõƒç√≠n', 'ostrava', 'praha', 'brno', 'teplice', '√∫st√≠']):
            return 'city_specific'
        
        # Dotazy o konkr√©tn√≠ dokumenty/t√©mata
        if any(word in query_lower for word in ['dokument', 'vyhl√°≈°ka', 'ozn√°men√≠', 'rozpoƒçet', 'na≈ô√≠zen√≠']):
            return 'document_specific'
        
        # ƒåasov√© dotazy
        if any(word in query_lower for word in ['nov√©', 'nejnovƒõj≈°√≠', 'aktu√°ln√≠', 'souƒçasn√©', 'dnes']):
            return 'temporal'
        
        # Procedur√°ln√≠ dotazy
        if any(word in query_lower for word in ['jak', 'kde', 'kdy≈æ', 'kdo', 'proƒç']):
            return 'procedural'
        
        return 'general'
    
    def _determine_search_depth(self, query_type: str, query: str) -> int:
        """Urƒç√≠ poƒçet dokument≈Ø k vyhled√°n√≠ podle typu dotazu"""
        depth_mapping = {
            'coverage': 8,      # Vysok√© pokryt√≠ pro obecn√© dotazy
            'city_specific': 5,  # St≈ôedn√≠ pokryt√≠ pro konkr√©tn√≠ obce
            'document_specific': 4,  # C√≠len√© vyhled√°n√≠
            'temporal': 6,      # V√≠ce dokument≈Ø pro ƒçasov√© anal√Ωzy
            'procedural': 3,    # M√©nƒõ dokument≈Ø, v√≠ce detailu
            'general': 3
        }
        return depth_mapping.get(query_type, 3)
    
    def _generate_no_results_response(self, query: str) -> str:
        """Generuje odpovƒõƒè kdy≈æ nejsou nalezeny dokumenty"""
        available_cities = set(doc.get('municipality', '') for doc in self.vector_store.documents)
        cities_list = ', '.join(sorted(available_cities))
        
        return f"""Omlouv√°m se, ale nena≈°el jsem relevantn√≠ dokumenty k va≈°√≠ ot√°zce: "{query}"

üìç **Dostupn√© obce v syst√©mu:**
{cities_list}

üí° **Tipy pro lep≈°√≠ vyhled√°v√°n√≠:**
- Zkuste upravit dotaz nebo pou≈æ√≠t jin√° kl√≠ƒçov√° slova
- Ptejte se na konkr√©tn√≠ obce nebo typy dokument≈Ø
- Pou≈æijte obecnƒõj≈°√≠ term√≠ny (nap≈ô. "ozn√°men√≠", "vyhl√°≈°ka")

‚ùì Mohu v√°m pomoci s nƒõƒç√≠m jin√Ωm t√Ωkaj√≠c√≠m se √∫≈ôedn√≠ch desek?"""

    def _build_enhanced_context(self, documents: List[Dict], query_type: str) -> str:
        """Sestav√≠ pokroƒçil√Ω kontext podle typu dotazu"""
        if query_type == 'coverage':
            # Pro dotazy o pokryt√≠ - z√≠sk√°me statistiky ze v≈°ech dokument≈Ø
            all_municipalities = {}
            for doc in self.vector_store.documents:  # Pou≈æijeme v≈°echny dokumenty, ne jen v√Ωsledky
                municipality = doc.get('municipality', 'Nezn√°m√° obec')
                all_municipalities[municipality] = all_municipalities.get(municipality, 0) + 1
            
            context_parts = ["=== KOMPLETN√ç P≈òEHLED V≈†ECH OBC√ç V SYST√âMU ==="]
            total_docs = sum(all_municipalities.values())
            context_parts.append(f"CELKEM DOKUMENT≈Æ: {total_docs}")
            context_parts.append("ROZLO≈ΩEN√ç PO OBC√çCH:")
            
            for municipality, count in sorted(all_municipalities.items()):
                percentage = (count / total_docs) * 100 if total_docs > 0 else 0
                context_parts.append(f"‚Ä¢ {municipality}: {count} dokument≈Ø ({percentage:.1f}%)")
            
            return "\n".join(context_parts)
        
        else:
            # Standardn√≠ kontext s roz≈°√≠≈ôen√Ωmi informacemi
            context_parts = []
            for i, doc in enumerate(documents, 1):
                title = doc.get('title', 'Dokument bez n√°zvu')
                municipality = doc.get('municipality', '')
                processed_date = doc.get('processed_at', '')
                url = doc.get('original_url', '')
                snippet = doc.get('text', '')[:600]
                
                # Zkr√°cen√© datum
                date_str = ""
                if processed_date:
                    try:
                        date_obj = datetime.fromisoformat(processed_date.replace('Z', '+00:00'))
                        date_str = f" ({date_obj.strftime('%d.%m.%Y')})"
                    except:
                        pass
                
                context_parts.append(
                    f"[üìÑ Dokument {i}]\n"
                    f"üìç Obec: {municipality}\n"
                    f"üìã N√°zev: {title}{date_str}\n"
                    f"üîó URL: {url[:80]}...\n"
                    f"üìù Obsah: {snippet}...\n"
                )
            
            return "\n".join(context_parts)
    
    def _build_adaptive_prompt(self, query: str, context: str, query_type: str) -> str:
        """Vytvo≈ô√≠ adaptivn√≠ prompt podle typu dotazu"""
        
        base_instructions = {
            'coverage': """√öKOL: Na z√°kladƒõ statistik v kontextu vypi≈° P≈òESN√ù poƒçet dokument≈Ø pro ka≈ædou obec.
FORM√ÅT: üìç Strukturovan√Ω seznam v≈°ech obc√≠ s P≈òESN√ùMI poƒçty z kontextu (ne odhadovan√©!).""",
            
            'city_specific': """√öKOL: Poskytni detailn√≠ informace o konkr√©tn√≠ obci.
FORM√ÅT: Konkr√©tn√≠ odpovƒõƒè s odkazy na relevantn√≠ dokumenty.""",
            
            'document_specific': """√öKOL: Najdi a shr≈à konkr√©tn√≠ dokumenty nebo t√©mata.
FORM√ÅT: Detailn√≠ p≈ôehled s typy dokument≈Ø a obsahem.""",
            
            'temporal': """√öKOL: Anal√Ωza podle ƒçasov√©ho hlediska.
FORM√ÅT: Chronologicky se≈ôazen√© informace s daty.""",
            
            'procedural': """√öKOL: Poskytni praktick√© pokyny a postupy.
FORM√ÅT: Krok za krokem n√°vod s d≈Øle≈æit√Ωmi informacemi.""",
            
            'general': """√öKOL: Obecn√° anal√Ωza dokument≈Ø.
FORM√ÅT: Strukturovan√° odpovƒõƒè s hlavn√≠mi body."""
        }
        
        instruction = base_instructions.get(query_type, base_instructions['general'])
        
        return f"""{instruction}

KONTEXT DOKUMENT≈Æ:
{context}

U≈ΩIVATELSK√ù DOTAZ: 
{query}

ODPOVƒöƒé (pou≈æij emotikony pro lep≈°√≠ ƒçitelnost):"""

    def _enhance_response(self, response: str, relevant_docs: List[Dict], query_type: str) -> str:
        """Vylep≈°√≠ odpovƒõƒè o dodateƒçn√© informace"""
        if not response or len(response.strip()) < 10:
            return "Omlouv√°m se, nepoda≈ôilo se mi generovat odpovƒõƒè na v√°≈° dotaz."
        
        # P≈ôid√°n√≠ metadat o zdroj√≠ch
        sources_info = f"\n\nüìö **Pou≈æit√© zdroje ({len(relevant_docs)} dokument≈Ø):**"
        for i, doc in enumerate(relevant_docs[:3], 1):  # Max 3 zdroje
            municipality = doc.get('municipality', 'Nezn√°m√° obec')
            title = doc.get('title', 'Bez n√°zvu')[:50]
            sources_info += f"\n{i}. {municipality}: {title}..."
        
        if len(relevant_docs) > 3:
            sources_info += f"\n... a {len(relevant_docs) - 3} dal≈°√≠ch dokument≈Ø"
        
        # P≈ôid√°n√≠ kontextov√Ωch tip≈Ø
        tips = {
            'city_specific': "\n\nüí° *Pro v√≠ce detail≈Ø o jin√Ωch obc√≠ch se zeptejte konkr√©tnƒõ.*",
            'coverage': "\n\nüí° *Pro detaily o konkr√©tn√≠ obci se zeptejte specificky.*",
            'document_specific': "\n\nüí° *Pot≈ôebujete v√≠ce informac√≠ o konkr√©tn√≠m dokumentu?*",
            'temporal': "\n\nüí° *M√°m nejnovƒõj≈°√≠ dostupn√© informace z √∫≈ôedn√≠ch desek.*"
        }
        
        tip = tips.get(query_type, "")
        
        return response.strip() + sources_info + tip

    def chat_loop(self):
        """Interaktivn√≠ chat smyƒçka"""
        print("\n" + "="*60)
        print("CHATBOT √ö≈òEDN√çCH DESEK (Gemma 3:4b)")
        print("="*60)
        print("Napi≈°te 'konec' pro ukonƒçen√≠\n")
        
        while True:
            try:
                user_input = input("Vy: ").strip()
                
                if user_input.lower() in ['konec', 'quit', 'exit']:
                    print("Nashledanou!")
                    break
                
                if not user_input:
                    continue
                
                response = self.generate_response(user_input)
                print(f"\nü§ñ Chatbot: {response}\n")
                
            except KeyboardInterrupt:
                print("\n\nNashledanou!")
                break
            except Exception as e:
                print(f"\n‚ùå Chyba: {e}\n")


# ============================================================================
# 4. ORCHESTRACE A HLAVN√ç PROGRAM
# ============================================================================

def main():
    """Hlavn√≠ program"""
    
    print("=" * 60)
    print("√ö≈òEDN√ç DESKA CHATBOT - SETUP (Gemma 3:4b)")
    print("=" * 60)
    
    # 1. Sbƒõr dat
    collector = NKODCollector()
    municipalities = ["Dƒõƒç√≠n", "√öst√≠ nad Labem", "Praha", "Brno", "Ostrava", "Teplice"]
    
    all_documents = []
    
    for municipality in municipalities:
        print(f"\n[1] Vyhled√°v√°m √∫≈ôedn√≠ desky pro {municipality}...")
        
        cached = collector.load_cached(municipality)
        if cached:
            print(f"  ‚úì Naƒçteno z cache: {len(cached)} datasety")
            metadata = cached
        else:
            metadata = collector.search_boards(municipality)
            if metadata:
                collector.cache_results(municipality, metadata)
                print(f"  ‚úì Nalezeno: {len(metadata)} datasety")
            else:
                print(f"  ‚úó Nic nenalezeno")
        
        # 2. Zpracov√°n√≠ dokument≈Ø
        if metadata:
            print(f"\n[2] Stahuju a zpracov√°v√°m dokumenty...")
            processor = DocumentProcessor()
            documents = processor.process_documents(metadata)
            all_documents.extend(documents)
            print(f"  ‚úì Zpracov√°no: {len(documents)} dokument≈Ø")
    
    # 3. Vytvo≈ô vektorov√© √∫lo≈æi≈°tƒõ
    print(f"\n[3] Vytv√°≈ô√≠m vektorov√© √∫lo≈æi≈°tƒõ...")
    vector_store = SimpleVectorStore()
    vector_store.add_documents(all_documents)
    print(f"  ‚úì Ulo≈æeno: {len(all_documents)} dokument≈Ø")
    
    # 4. Inicializuj chatbot
    print(f"\n[4] Inicializuji chatbot s Gemma 3:4b...")
    chatbot = ChatbotRAG(vector_store, model="gemma3:4b")
    
    # 5. Spus≈• interaktivn√≠ chat
    chatbot.chat_loop()


if __name__ == "__main__":
    main()